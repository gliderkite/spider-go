// Defines all the events that can be generated by the crawler.
package spider

import "time"

// Represents a crawling main event.
type Event interface {
	// Return the unique ID of the entity (crawler) that generated the event.
	Id() string
	// Returns the time when the event occurred.
	When() time.Time
}

// Crawling event basic information.
type eventInfo struct {
	id   string
	time time.Time
}

// Signal that the crawling started.
type CrawlingStarted struct {
	info eventInfo
}

func newCrawlingStarted(id string) *CrawlingStarted {
	return &CrawlingStarted{eventInfo{id, time.Now()}}
}

func (event *CrawlingStarted) Id() string {
	return event.info.id
}

func (event *CrawlingStarted) When() time.Time {
	return event.info.time
}

// Signal that the crawling ended.
type CrawlingEnded struct {
	info eventInfo
	// Unique number of visited URLs.
	VisitedCount int
}

func newCrawlingEnded(id string, visitedCount int) *CrawlingEnded {
	return &CrawlingEnded{eventInfo{id, time.Now()}, visitedCount}
}

func (event *CrawlingEnded) Id() string {
	return event.info.id
}

func (event *CrawlingEnded) When() time.Time {
	return event.info.time
}

// Signal that the given argument was not valid.
type InvalidArg struct {
	info eventInfo
	err  error
}

func newInvalidArg(id string, err error) *InvalidArg {
	return &InvalidArg{eventInfo{id, time.Now()}, err}
}

func (event *InvalidArg) Id() string {
	return event.info.id
}

func (event *InvalidArg) When() time.Time {
	return event.info.time
}

func (event *InvalidArg) Error() error {
	return event.err
}

// Signal that a page has been visited.
type PageVisited struct {
	info eventInfo
	Page *pageUrls
}

func newPageVisited(id string, page *pageUrls) *PageVisited {
	return &PageVisited{eventInfo{id, time.Now()}, page}
}

func (event *PageVisited) Id() string {
	return event.info.id
}

func (event *PageVisited) When() time.Time {
	return event.info.time
}
